from app.services.adapter import TokenizerAdapter

DEFAULT_SAMPLE_TEXTS = [
    "The quick brown fox jumps over the lazy dog.",
    "Machine learning models process natural language by breaking text into tokens.",
    "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
    "El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso.",
    "æ—¥æœ¬èªžã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ãƒ†ã‚¹ãƒˆã€‚",
    "ÐŸÑ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€! Ð­Ñ‚Ð¾ Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.",
    "SELECT * FROM users WHERE age > 18 ORDER BY name;",
    "https://example.com/path?key=value&other=123#section",
    "Hello! ðŸ˜Š How are you doing today? I'm great! ðŸŽ‰",
    "3.14159265358979323846264338327950288",
]


def compute_overlap(adapters: dict[str, TokenizerAdapter]) -> dict:
    """Compute vocabulary overlap between tokenizers."""
    vocab_sets: dict[str, set[str]] = {}
    for tok_id, adapter in adapters.items():
        vocab_sets[tok_id] = set(adapter.get_vocab().keys())

    all_ids = list(vocab_sets.keys())

    # Compute intersection of all
    shared = set.intersection(*vocab_sets.values()) if vocab_sets else set()
    union = set.union(*vocab_sets.values()) if vocab_sets else set()

    # Unique per tokenizer
    unique_per = {}
    for tok_id, vocab in vocab_sets.items():
        others = set.union(*(v for k, v in vocab_sets.items() if k != tok_id))
        unique_per[tok_id] = len(vocab - others)

    overlap_pct = (len(shared) / max(len(union), 1)) * 100

    return {
        "shared_tokens": len(shared),
        "unique_per_tokenizer": unique_per,
        "total_union": len(union),
        "overlap_percentage": round(overlap_pct, 2),
        "shared_sample": sorted(list(shared))[:50],
        "unique_samples": {
            tok_id: sorted(list(vocab_sets[tok_id] - shared))[:30]
            for tok_id in all_ids
        },
    }


def compare_tokenization(
    adapters: dict[str, TokenizerAdapter], text: str
) -> list[dict]:
    """Compare how different tokenizers tokenize the same text."""
    results = []
    for tok_id, adapter in adapters.items():
        token_ids = adapter.encode(text)
        tokens = []
        for tid in token_ids:
            token_str = adapter.decode_single(tid)
            token_bytes = token_str.encode("utf-8", errors="replace")
            tokens.append(
                {
                    "id": tid,
                    "token_str": token_str,
                    "token_bytes_hex": token_bytes.hex(),
                    "byte_length": len(token_bytes),
                }
            )
        results.append(
            {
                "tokenizer_id": tok_id,
                "tokens": tokens,
                "token_count": len(tokens),
            }
        )
    return results


def compute_efficiency(
    adapters: dict[str, TokenizerAdapter],
    sample_texts: list[str] | None = None,
) -> list[dict]:
    """Compare tokenization efficiency across tokenizers."""
    texts = sample_texts or DEFAULT_SAMPLE_TEXTS

    results = []
    for tok_id, adapter in adapters.items():
        total_tokens = 0
        total_chars = 0
        total_words = 0

        for text in texts:
            token_ids = adapter.encode(text)
            total_tokens += len(token_ids)
            total_chars += len(text)
            total_words += len(text.split())

        avg_tokens_per_word = total_tokens / max(total_words, 1)
        avg_token_length = total_chars / max(total_tokens, 1)

        results.append(
            {
                "tokenizer_id": tok_id,
                "avg_tokens_per_word": round(avg_tokens_per_word, 3),
                "avg_token_length_chars": round(avg_token_length, 3),
                "total_tokens": total_tokens,
                "total_chars": total_chars,
            }
        )

    return results
